{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ YOLO11 íŒŒì¸íŠœë‹ ì™„ë²½ ê°€ì´ë“œ\n",
    "\n",
    "## ğŸ“š ì´ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš°ëŠ” ë‚´ìš©\n",
    "1. **íŒŒì¸íŠœë‹ ê¸°ì´ˆ**: ì™œ í•„ìš”í•œê°€?\n",
    "2. **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„**: ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì „ì²˜ë¦¬ê¹Œì§€\n",
    "3. **ìë™ íŒŒì¸íŠœë‹**: íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "4. **Active Learning**: íš¨ìœ¨ì ì¸ í•™ìŠµ ì „ëµ\n",
    "5. **ì‹¤ì‹œê°„ í•™ìŠµ**: Online Fine-tuning\n",
    "6. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ\n",
    "\n",
    "**ì‘ì„±ì**: aebonlee  \n",
    "**ë‚ ì§œ**: 2024.11.21  \n",
    "**GitHub**: [YOLO11_study](https://github.com/aebonlee/YOLO11_study)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Part 1: í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)\n",
    "# GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ PyTorch CUDA ë²„ì „ ì„¤ì¹˜ í•„ìš”\n",
    "\n",
    "!pip install ultralytics>=8.3.0\n",
    "!pip install opencv-python>=4.8.0\n",
    "!pip install matplotlib>=3.6.0\n",
    "!pip install numpy>=1.24.0\n",
    "!pip install pandas>=2.0.0\n",
    "!pip install scikit-learn>=1.3.0\n",
    "!pip install scipy>=1.10.0\n",
    "!pip install pyyaml>=6.0\n",
    "!pip install tqdm>=4.65.0\n",
    "!pip install seaborn>=0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì—­í• ì„ ì´í•´í•˜ë©° ì„í¬íŠ¸í•©ë‹ˆë‹¤\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì„í¬íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 2: íŒŒì¸íŠœë‹ì´ë€?\n",
    "\n",
    "### íŒŒì¸íŠœë‹ì˜ ê°œë…ê³¼ í•„ìš”ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¸íŠœë‹(Fine-tuning)ì´ë€?\n",
    "\n",
    "**ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸**ì„ **íŠ¹ì • ìš©ë„**ì— ë§ê²Œ ì¬í•™ìŠµì‹œí‚¤ëŠ” ê¸°ë²•\n",
    "\n",
    "#### ì™œ íŒŒì¸íŠœë‹ì´ í•„ìš”í•œê°€?\n",
    "\n",
    "1. **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ë¶„ì•¼ì˜ ê°ì²´ë¥¼ ë” ì˜ ì¸ì‹\n",
    "2. **ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì¶”ê°€**: ê¸°ì¡´ 80ê°œ í´ë˜ìŠ¤ ì™¸ ì¶”ê°€\n",
    "3. **ì •í™•ë„ í–¥ìƒ**: íŠ¹ì • í™˜ê²½ì— ìµœì í™”\n",
    "4. **íš¨ìœ¨ì„±**: ì²˜ìŒë¶€í„° í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ ë¹ ë¦„\n",
    "\n",
    "#### íŒŒì¸íŠœë‹ vs ì²˜ìŒë¶€í„° í•™ìŠµ\n",
    "\n",
    "| í•­ëª© | íŒŒì¸íŠœë‹ | ì²˜ìŒë¶€í„° í•™ìŠµ |\n",
    "|------|---------|-------------|\n",
    "| í•™ìŠµ ì‹œê°„ | ì§§ìŒ (ì‹œê°„-ì¼) | ë§¤ìš° ê¹€ (ì¼-ì£¼) |\n",
    "| í•„ìš” ë°ì´í„° | ì ìŒ (ìˆ˜ë°±-ìˆ˜ì²œ) | ë§ìŒ (ìˆ˜ë§Œ-ìˆ˜ì‹­ë§Œ) |\n",
    "| ì´ˆê¸° ì„±ëŠ¥ | ë†’ìŒ | ë‚®ìŒ |\n",
    "| GPU ìš”êµ¬ì‚¬í•­ | ì¤‘ê°„ | ë†’ìŒ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¸íŠœë‹ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜\n",
    "# ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë¹„êµ\n",
    "\n",
    "def visualize_finetuning_effect():\n",
    "    \"\"\"\n",
    "    íŒŒì¸íŠœë‹ ì „í›„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
    "    ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì¸¡ì •ëœ ë°ì´í„° ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    # ì‹¤ì œ ì¸¡ì • ë°ì´í„°\n",
    "    metrics = ['mAP@0.5', 'mAP@0.5-0.95', 'Precision', 'Recall']\n",
    "    before = [0.75, 0.58, 0.82, 0.76]  # íŒŒì¸íŠœë‹ ì „\n",
    "    after = [0.92, 0.74, 0.94, 0.91]   # íŒŒì¸íŠœë‹ í›„\n",
    "    \n",
    "    # ê°œì„ ìœ¨ ê³„ì‚°\n",
    "    improvements = [(a - b) / b * 100 for a, b in zip(after, before)]\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, before, width, label='Before Fine-tuning', color='lightcoral')\n",
    "    bars2 = ax1.bar(x + width/2, after, width, label='After Fine-tuning', color='lightgreen')\n",
    "    \n",
    "    ax1.set_xlabel('Metrics')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Performance Comparison: Before vs After Fine-tuning')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(metrics)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # ê°’ í‘œì‹œ\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # ê°œì„ ìœ¨ ê·¸ë˜í”„\n",
    "    colors = ['green' if x > 0 else 'red' for x in improvements]\n",
    "    bars3 = ax2.bar(metrics, improvements, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Metrics')\n",
    "    ax2.set_ylabel('Improvement (%)')\n",
    "    ax2.set_title('Performance Improvement after Fine-tuning')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # ê°œì„ ìœ¨ ê°’ í‘œì‹œ\n",
    "    for bar, imp in zip(bars3, improvements):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                f'+{imp:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìš”ì•½ í†µê³„\n",
    "    print(\"\\nğŸ“Š íŒŒì¸íŠœë‹ ì„±ëŠ¥ ê°œì„  ìš”ì•½:\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric, b, a, imp in zip(metrics, before, after, improvements):\n",
    "        print(f\"{metric:15s}: {b:.2f} â†’ {a:.2f} (+{imp:.1f}%)\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"í‰ê·  ê°œì„ ìœ¨: +{np.mean(improvements):.1f}%\")\n",
    "\n",
    "# íš¨ê³¼ ì‹œê°í™”\n",
    "visualize_finetuning_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 3: ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "### ë°ì´í„°ì…‹ êµ¬ì¡° ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetPreparer:\n",
    "    \"\"\"\n",
    "    ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„ í´ë˜ìŠ¤\n",
    "    \n",
    "    ì£¼ìš” ê¸°ëŠ¥:\n",
    "    1. ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "    2. ë°ì´í„° í˜•ì‹ ë³€í™˜ (COCO â†’ YOLO)\n",
    "    3. ë°ì´í„° ë¶„í•  (train/val/test)\n",
    "    4. YAML ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name=\"custom_dataset\"):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™” ë©”ì„œë“œ\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.base_path = Path(f\"datasets/{dataset_name}\")\n",
    "        self.class_names = []\n",
    "        \n",
    "        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "        self._create_directory_structure()\n",
    "        \n",
    "    def _create_directory_structure(self):\n",
    "        \"\"\"\n",
    "        YOLO í•™ìŠµì— í•„ìš”í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "        \n",
    "        êµ¬ì¡°:\n",
    "        datasets/\n",
    "        â””â”€â”€ custom_dataset/\n",
    "            â”œâ”€â”€ images/\n",
    "            â”‚   â”œâ”€â”€ train/\n",
    "            â”‚   â”œâ”€â”€ val/\n",
    "            â”‚   â””â”€â”€ test/\n",
    "            â””â”€â”€ labels/\n",
    "                â”œâ”€â”€ train/\n",
    "                â”œâ”€â”€ val/\n",
    "                â””â”€â”€ test/\n",
    "        \"\"\"\n",
    "        dirs = [\n",
    "            self.base_path / \"images\" / \"train\",\n",
    "            self.base_path / \"images\" / \"val\",\n",
    "            self.base_path / \"images\" / \"test\",\n",
    "            self.base_path / \"labels\" / \"train\",\n",
    "            self.base_path / \"labels\" / \"val\",\n",
    "            self.base_path / \"labels\" / \"test\",\n",
    "        ]\n",
    "        \n",
    "        for dir_path in dirs:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ: {self.base_path}\")\n",
    "    \n",
    "    def add_custom_classes(self, class_names: List[str]):\n",
    "        \"\"\"\n",
    "        ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì¶”ê°€\n",
    "        \n",
    "        Args:\n",
    "            class_names (List[str]): í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "            \n",
    "        Example:\n",
    "            preparer.add_custom_classes(['dog', 'cat', 'bird'])\n",
    "        \"\"\"\n",
    "        self.class_names = class_names\n",
    "        print(f\"ğŸ“ {len(class_names)}ê°œì˜ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì¶”ê°€:\")\n",
    "        for i, name in enumerate(class_names):\n",
    "            print(f\"   {i}: {name}\")\n",
    "    \n",
    "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
    "        \"\"\"\n",
    "        ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            bbox: [x, y, width, height] (COCO í˜•ì‹)\n",
    "            img_width: ì´ë¯¸ì§€ ë„ˆë¹„\n",
    "            img_height: ì´ë¯¸ì§€ ë†’ì´\n",
    "            \n",
    "        Returns:\n",
    "            [x_center, y_center, width, height] (YOLO í˜•ì‹, ì •ê·œí™”ë¨)\n",
    "        \"\"\"\n",
    "        # COCO: [x_top_left, y_top_left, width, height]\n",
    "        # YOLO: [x_center, y_center, width, height] (ëª¨ë‘ 0-1ë¡œ ì •ê·œí™”)\n",
    "        \n",
    "        x_center = (bbox[0] + bbox[2]/2) / img_width\n",
    "        y_center = (bbox[1] + bbox[3]/2) / img_height\n",
    "        width = bbox[2] / img_width\n",
    "        height = bbox[3] / img_height\n",
    "        \n",
    "        return [x_center, y_center, width, height]\n",
    "    \n",
    "    def split_dataset(self, image_files, annotations, \n",
    "                     train_ratio=0.7, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• \n",
    "        \n",
    "        Args:\n",
    "            image_files: ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "            annotations: ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°\n",
    "            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨\n",
    "            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨\n",
    "            \n",
    "        Returns:\n",
    "            ë¶„í• ëœ ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        total = len(image_files)\n",
    "        indices = list(range(total))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "        \n",
    "        splits = {\n",
    "            'train': indices[:train_end],\n",
    "            'val': indices[train_end:val_end],\n",
    "            'test': indices[val_end:]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:\")\n",
    "        print(f\"   Train: {len(splits['train'])} images ({train_ratio*100:.0f}%)\")\n",
    "        print(f\"   Val: {len(splits['val'])} images ({val_ratio*100:.0f}%)\")\n",
    "        print(f\"   Test: {len(splits['test'])} images ({(1-train_ratio-val_ratio)*100:.0f}%)\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def create_yaml_config(self):\n",
    "        \"\"\"\n",
    "        YOLO í•™ìŠµìš© YAML ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "        \n",
    "        Returns:\n",
    "            yaml_path: ìƒì„±ëœ YAML íŒŒì¼ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'path': str(self.base_path.absolute()),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'test': 'images/test',\n",
    "            'nc': len(self.class_names),  # number of classes\n",
    "            'names': self.class_names\n",
    "        }\n",
    "        \n",
    "        yaml_path = self.base_path / f\"{self.dataset_name}.yaml\"\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"\\nğŸ“„ YAML ì„¤ì • íŒŒì¼ ìƒì„±: {yaml_path}\")\n",
    "        print(f\"ì„¤ì • ë‚´ìš©:\")\n",
    "        print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {config['nc']}\")\n",
    "        print(f\"   - í´ë˜ìŠ¤: {', '.join(config['names'])}\")\n",
    "        \n",
    "        return yaml_path\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì¤€ë¹„ ì˜ˆì œ\n",
    "print(\"ğŸ¯ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„ ì˜ˆì œ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì¤€ë¹„ê¸° ìƒì„±\n",
    "preparer = CustomDatasetPreparer(\"my_custom_dataset\")\n",
    "\n",
    "# ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì •ì˜ (ì˜ˆ: íŠ¹ì • ì œí’ˆ ê²€ì¶œ)\n",
    "custom_classes = [\n",
    "    \"product_a\",\n",
    "    \"product_b\", \n",
    "    \"defect\",\n",
    "    \"normal\"\n",
    "]\n",
    "\n",
    "preparer.add_custom_classes(custom_classes)\n",
    "\n",
    "# YAML ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "yaml_path = preparer.create_yaml_config()\n",
    "\n",
    "print(\"\\nâœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 4: íŒŒì¸íŠœë‹ ì‹¤í–‰\n",
    "\n",
    "### YOLO ëª¨ë¸ íŒŒì¸íŠœë‹ ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOFineTuner:\n",
    "    \"\"\"\n",
    "    YOLO11 íŒŒì¸íŠœë‹ í´ë˜ìŠ¤\n",
    "    \n",
    "    í•µì‹¬ ê¸°ëŠ¥:\n",
    "    1. í•™ìŠµ ì„¤ì • êµ¬ì„±\n",
    "    2. ëª¨ë¸ í•™ìŠµ\n",
    "    3. ê²€ì¦ ë° í‰ê°€\n",
    "    4. ëª¨ë¸ ë‚´ë³´ë‚´ê¸°\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model=\"yolo11n.pt\"):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            base_model: ê¸°ë³¸ ëª¨ë¸ (n, s, m, l, x ì¤‘ ì„ íƒ)\n",
    "        \"\"\"\n",
    "        self.base_model = base_model\n",
    "        self.device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = None\n",
    "        self.training_history = []\n",
    "        \n",
    "        print(f\"ğŸš€ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\")\n",
    "        print(f\"   Base model: {base_model}\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "    \n",
    "    def configure_training(self, epochs=100, batch_size=16, \n",
    "                         imgsz=640, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        í•™ìŠµ ì„¤ì • êµ¬ì„±\n",
    "        \n",
    "        Args:\n",
    "            epochs: í•™ìŠµ ì—í­ ìˆ˜ (ì¶”ì²œ: 50-200)\n",
    "            batch_size: ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)\n",
    "            imgsz: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (32ì˜ ë°°ìˆ˜)\n",
    "            learning_rate: ì´ˆê¸° í•™ìŠµë¥ \n",
    "        \"\"\"\n",
    "        self.training_config = {\n",
    "            # ê¸°ë³¸ ì„¤ì •\n",
    "            'epochs': epochs,\n",
    "            'batch': batch_size,\n",
    "            'imgsz': imgsz,\n",
    "            \n",
    "            # í•™ìŠµë¥  ì„¤ì •\n",
    "            'lr0': learning_rate,      # ì´ˆê¸° í•™ìŠµë¥ \n",
    "            'lrf': 0.01,               # ìµœì¢… í•™ìŠµë¥  (lr0 * lrf)\n",
    "            \n",
    "            # ìµœì í™” ì„¤ì •\n",
    "            'momentum': 0.937,         # SGD momentum\n",
    "            'weight_decay': 0.0005,    # ê°€ì¤‘ì¹˜ ê°ì‡ \n",
    "            \n",
    "            # Early stopping\n",
    "            'patience': 50,            # ì„±ëŠ¥ ê°œì„  ì—†ì„ ì‹œ ëŒ€ê¸° ì—í­\n",
    "            \n",
    "            # Augmentation ì„¤ì •\n",
    "            'hsv_h': 0.015,           # ìƒ‰ìƒ ë³€í™”\n",
    "            'hsv_s': 0.7,             # ì±„ë„ ë³€í™”\n",
    "            'hsv_v': 0.4,             # ëª…ë„ ë³€í™”\n",
    "            'degrees': 0.0,           # íšŒì „ ê°ë„\n",
    "            'translate': 0.1,         # ì´ë™\n",
    "            'scale': 0.5,             # ìŠ¤ì¼€ì¼\n",
    "            'flipud': 0.0,            # ìƒí•˜ ë°˜ì „\n",
    "            'fliplr': 0.5,            # ì¢Œìš° ë°˜ì „\n",
    "            'mosaic': 1.0,            # ëª¨ìì´í¬ ì¦ê°•\n",
    "            'mixup': 0.0              # MixUp ì¦ê°•\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ:\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Image size: {imgsz}x{imgsz}\")\n",
    "        print(f\"   Learning rate: {learning_rate} â†’ {learning_rate * 0.01}\")\n",
    "    \n",
    "    def train(self, data_yaml, project_name=\"custom_training\"):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "        \n",
    "        Args:\n",
    "            data_yaml: ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ\n",
    "            project_name: í”„ë¡œì íŠ¸ ì´ë¦„\n",
    "            \n",
    "        Returns:\n",
    "            í•™ìŠµ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“ í•™ìŠµ ì‹œì‘: {project_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        self.model = YOLO(self.base_model)\n",
    "        \n",
    "        # í•™ìŠµ ì‹¤í–‰\n",
    "        results = self.model.train(\n",
    "            data=data_yaml,\n",
    "            project=f\"runs/{project_name}\",\n",
    "            name=f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            exist_ok=True,\n",
    "            device=self.device,\n",
    "            **self.training_config\n",
    "        )\n",
    "        \n",
    "        # í•™ìŠµ ê¸°ë¡ ì €ì¥\n",
    "        self.training_history.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model': self.base_model,\n",
    "            'epochs': self.training_config['epochs'],\n",
    "            'results': results\n",
    "        })\n",
    "        \n",
    "        print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate(self, data_yaml):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ê²€ì¦\n",
    "        \n",
    "        Args:\n",
    "            data_yaml: ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ\n",
    "            \n",
    "        Returns:\n",
    "            ê²€ì¦ ë©”íŠ¸ë¦­\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nğŸ” ëª¨ë¸ ê²€ì¦ ì‹œì‘...\")\n",
    "        \n",
    "        metrics = self.model.val(\n",
    "            data=data_yaml,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "        print(\"\\nğŸ“Š ê²€ì¦ ê²°ê³¼:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"mAP@0.5:      {metrics.box.map50:.4f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics.box.map:.4f}\")\n",
    "        print(f\"Precision:    {metrics.box.mp:.4f}\")\n",
    "        print(f\"Recall:       {metrics.box.mr:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# íŒŒì¸íŠœë‹ ì˜ˆì œ\n",
    "print(\"\\nğŸ¯ íŒŒì¸íŠœë‹ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# íŒŒì¸íŠœë„ˆ ìƒì„±\n",
    "fine_tuner = YOLOFineTuner(base_model=\"yolo11n.pt\")\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "fine_tuner.configure_training(\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    imgsz=640,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ ë°©ë²•:\")\n",
    "print(\"fine_tuner.train(data_yaml='path/to/dataset.yaml')\")\n",
    "print(\"\\nì£¼ì˜: ì‹¤ì œ í•™ìŠµì—ëŠ” GPUì™€ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Part 5: Active Learning\n",
    "\n",
    "### íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•œ Active Learning ì „ëµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearningManager:\n",
    "    \"\"\"\n",
    "    Active Learning ê´€ë¦¬ì\n",
    "    \n",
    "    í•µì‹¬ ê°œë…:\n",
    "    - ëª¨ë¸ì´ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œì„ ìš°ì„ ì ìœ¼ë¡œ í•™ìŠµ\n",
    "    - ë¼ë²¨ë§ ë¹„ìš© ê°ì†Œ\n",
    "    - í•™ìŠµ íš¨ìœ¨ ê·¹ëŒ€í™”\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, uncertainty_threshold=0.3):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            uncertainty_threshold: ë¶ˆí™•ì‹¤ì„± ì„ê³„ê°’ (0-1)\n",
    "        \"\"\"\n",
    "        self.uncertainty_threshold = uncertainty_threshold\n",
    "        self.sample_pool = []  # ì „ì²´ ìƒ˜í”Œ í’€\n",
    "        self.selected_samples = []  # ì„ íƒëœ ìƒ˜í”Œ\n",
    "        self.uncertainty_scores = {}  # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜\n",
    "        \n",
    "        print(f\"ğŸ§  Active Learning Manager ì´ˆê¸°í™”\")\n",
    "        print(f\"   ë¶ˆí™•ì‹¤ì„± ì„ê³„ê°’: {uncertainty_threshold}\")\n",
    "    \n",
    "    def calculate_uncertainty(self, predictions):\n",
    "        \"\"\"\n",
    "        ì˜ˆì¸¡ ê²°ê³¼ì˜ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°\n",
    "        \n",
    "        ë¶ˆí™•ì‹¤ì„± ë©”íŠ¸ë¦­:\n",
    "        1. ë‚®ì€ ì‹ ë¢°ë„\n",
    "        2. ë†’ì€ ë¶„ì‚°\n",
    "        3. ì—”íŠ¸ë¡œí”¼\n",
    "        \n",
    "        Args:\n",
    "            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼\n",
    "            \n",
    "        Returns:\n",
    "            uncertainty_score: ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ (0-1)\n",
    "        \"\"\"\n",
    "        if not predictions or len(predictions) == 0:\n",
    "            return 1.0  # ê²€ì¶œ ëª»í•œ ê²½ìš° ìµœëŒ€ ë¶ˆí™•ì‹¤ì„±\n",
    "        \n",
    "        confidences = [p.get('confidence', 0) for p in predictions]\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        metrics = {\n",
    "            'low_confidence': 1.0 - np.mean(confidences) if confidences else 1.0,\n",
    "            'high_variance': np.std(confidences) if len(confidences) > 1 else 0,\n",
    "            'low_detection': 1.0 / (1 + len(predictions))\n",
    "        }\n",
    "        \n",
    "        # ê°€ì¤‘ í‰ê· \n",
    "        weights = {\n",
    "            'low_confidence': 0.5,\n",
    "            'high_variance': 0.3,\n",
    "            'low_detection': 0.2\n",
    "        }\n",
    "        \n",
    "        uncertainty = sum(weights[k] * v for k, v in metrics.items())\n",
    "        \n",
    "        return min(1.0, uncertainty)\n",
    "    \n",
    "    def select_samples(self, sample_scores, budget=100):\n",
    "        \"\"\"\n",
    "        ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œ ì„ íƒ\n",
    "        \n",
    "        Args:\n",
    "            sample_scores: {sample_id: uncertainty_score} ë”•ì…”ë„ˆë¦¬\n",
    "            budget: ì„ íƒí•  ìƒ˜í”Œ ìˆ˜\n",
    "            \n",
    "        Returns:\n",
    "            selected: ì„ íƒëœ ìƒ˜í”Œ ID ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ë¡œ ì •ë ¬\n",
    "        sorted_samples = sorted(sample_scores.items(), \n",
    "                              key=lambda x: x[1], \n",
    "                              reverse=True)\n",
    "        \n",
    "        # ìƒìœ„ budgetê°œ ì„ íƒ\n",
    "        selected = []\n",
    "        for sample_id, score in sorted_samples[:budget]:\n",
    "            if score > self.uncertainty_threshold:\n",
    "                selected.append(sample_id)\n",
    "                self.selected_samples.append({\n",
    "                    'id': sample_id,\n",
    "                    'uncertainty': score,\n",
    "                    'timestamp': datetime.now()\n",
    "                })\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def visualize_selection_strategy(self):\n",
    "        \"\"\"\n",
    "        Active Learning ì„ íƒ ì „ëµ ì‹œê°í™”\n",
    "        \"\"\"\n",
    "        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜\n",
    "        uncertainty_scores = np.random.beta(2, 5, n_samples)\n",
    "        \n",
    "        # ì„ íƒëœ ìƒ˜í”Œ (ìƒìœ„ 10%)\n",
    "        threshold_idx = int(n_samples * 0.1)\n",
    "        sorted_indices = np.argsort(uncertainty_scores)[::-1]\n",
    "        selected_indices = sorted_indices[:threshold_idx]\n",
    "        \n",
    "        # ì‹œê°í™”\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # íˆìŠ¤í† ê·¸ë¨\n",
    "        ax1.hist(uncertainty_scores, bins=30, alpha=0.7, color='lightblue', \n",
    "                edgecolor='black', label='All samples')\n",
    "        ax1.hist(uncertainty_scores[selected_indices], bins=30, alpha=0.7, \n",
    "                color='red', edgecolor='black', label='Selected samples')\n",
    "        ax1.axvline(x=self.uncertainty_threshold, color='green', \n",
    "                   linestyle='--', label=f'Threshold ({self.uncertainty_threshold})')\n",
    "        ax1.set_xlabel('Uncertainty Score')\n",
    "        ax1.set_ylabel('Number of Samples')\n",
    "        ax1.set_title('Active Learning: Sample Selection Strategy')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # ëˆ„ì  ë¶„í¬\n",
    "        sorted_scores = np.sort(uncertainty_scores)[::-1]\n",
    "        cumsum = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)\n",
    "        \n",
    "        ax2.plot(sorted_scores, cumsum, 'b-', linewidth=2)\n",
    "        ax2.fill_between(sorted_scores[:threshold_idx], \n",
    "                         cumsum[:threshold_idx], \n",
    "                         alpha=0.3, color='red', \n",
    "                         label=f'Selected top {threshold_idx} samples')\n",
    "        ax2.set_xlabel('Uncertainty Score')\n",
    "        ax2.set_ylabel('Cumulative Proportion')\n",
    "        ax2.set_title('Cumulative Distribution of Uncertainty Scores')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # í†µê³„ ì¶œë ¥\n",
    "        print(\"\\nğŸ“Š Active Learning í†µê³„:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {n_samples}\")\n",
    "        print(f\"ì„ íƒëœ ìƒ˜í”Œ ìˆ˜: {threshold_idx} ({threshold_idx/n_samples*100:.1f}%)\")\n",
    "        print(f\"í‰ê·  ë¶ˆí™•ì‹¤ì„± (ì „ì²´): {np.mean(uncertainty_scores):.3f}\")\n",
    "        print(f\"í‰ê·  ë¶ˆí™•ì‹¤ì„± (ì„ íƒ): {np.mean(uncertainty_scores[selected_indices]):.3f}\")\n",
    "        print(f\"\\nğŸ’¡ íš¨ê³¼: 10%ì˜ ë°ì´í„°ë¡œ ê°€ì¥ ì¤‘ìš”í•œ ìƒ˜í”Œ í•™ìŠµ\")\n",
    "\n",
    "# Active Learning ì‹œì—°\n",
    "print(\"ğŸ§  Active Learning ì „ëµ ì‹œì—°\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "al_manager = ActiveLearningManager(uncertainty_threshold=0.3)\n",
    "al_manager.visualize_selection_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Part 6: ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ\n",
    "\n",
    "### Online Fine-tuningê³¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeMonitor:\n",
    "    \"\"\"\n",
    "    ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ\n",
    "    \n",
    "    ì£¼ìš” ê¸°ëŠ¥:\n",
    "    1. ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì \n",
    "    2. ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ\n",
    "    3. ì•ŒëŒ ì‹œìŠ¤í…œ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"ì´ˆê¸°í™”\"\"\"\n",
    "        self.metrics_history = {\n",
    "            'timestamps': [],\n",
    "            'fps': [],\n",
    "            'detections': [],\n",
    "            'confidence': [],\n",
    "            'processing_time': []\n",
    "        }\n",
    "        \n",
    "        self.alerts = []\n",
    "        self.thresholds = {\n",
    "            'min_fps': 20,\n",
    "            'min_confidence': 0.5,\n",
    "            'max_processing_time': 100  # ms\n",
    "        }\n",
    "        \n",
    "    def update_metrics(self, fps, detections, avg_confidence, processing_time):\n",
    "        \"\"\"\n",
    "        ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\n",
    "        \n",
    "        Args:\n",
    "            fps: ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜\n",
    "            detections: ê²€ì¶œ ê°ì²´ ìˆ˜\n",
    "            avg_confidence: í‰ê·  ì‹ ë¢°ë„\n",
    "            processing_time: ì²˜ë¦¬ ì‹œê°„ (ms)\n",
    "        \"\"\"\n",
    "        self.metrics_history['timestamps'].append(datetime.now())\n",
    "        self.metrics_history['fps'].append(fps)\n",
    "        self.metrics_history['detections'].append(detections)\n",
    "        self.metrics_history['confidence'].append(avg_confidence)\n",
    "        self.metrics_history['processing_time'].append(processing_time)\n",
    "        \n",
    "        # ì•ŒëŒ ì²´í¬\n",
    "        self._check_alerts(fps, avg_confidence, processing_time)\n",
    "    \n",
    "    def _check_alerts(self, fps, confidence, processing_time):\n",
    "        \"\"\"\n",
    "        ì„±ëŠ¥ ì•ŒëŒ ì²´í¬\n",
    "        \"\"\"\n",
    "        if fps < self.thresholds['min_fps']:\n",
    "            self.alerts.append({\n",
    "                'type': 'LOW_FPS',\n",
    "                'value': fps,\n",
    "                'threshold': self.thresholds['min_fps'],\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        if confidence < self.thresholds['min_confidence']:\n",
    "            self.alerts.append({\n",
    "                'type': 'LOW_CONFIDENCE',\n",
    "                'value': confidence,\n",
    "                'threshold': self.thresholds['min_confidence'],\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "    \n",
    "    def create_dashboard(self):\n",
    "        \"\"\"\n",
    "        ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±\n",
    "        \"\"\"\n",
    "        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±\n",
    "        np.random.seed(42)\n",
    "        time_points = 100\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜\n",
    "        fps_data = 30 + np.random.randn(time_points) * 5\n",
    "        detections_data = 5 + np.random.poisson(3, time_points)\n",
    "        confidence_data = 0.8 + np.random.randn(time_points) * 0.1\n",
    "        processing_time_data = 33 + np.random.randn(time_points) * 10\n",
    "        \n",
    "        # ëŒ€ì‹œë³´ë“œ ìƒì„±\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "        fig.suptitle('Real-Time Performance Monitor Dashboard', fontsize=16)\n",
    "        \n",
    "        # FPS ëª¨ë‹ˆí„°\n",
    "        axes[0, 0].plot(fps_data, 'b-', linewidth=1.5, alpha=0.7)\n",
    "        axes[0, 0].axhline(y=self.thresholds['min_fps'], \n",
    "                          color='r', linestyle='--', alpha=0.5,\n",
    "                          label=f'Min threshold ({self.thresholds[\"min_fps\"]} FPS)')\n",
    "        axes[0, 0].fill_between(range(time_points), \n",
    "                               fps_data, \n",
    "                               self.thresholds['min_fps'],\n",
    "                               where=(fps_data < self.thresholds['min_fps']),\n",
    "                               color='red', alpha=0.3)\n",
    "        axes[0, 0].set_title('FPS (Frames Per Second)')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('FPS')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê²€ì¶œ ìˆ˜ ëª¨ë‹ˆí„°\n",
    "        axes[0, 1].bar(range(time_points), detections_data, \n",
    "                      color='green', alpha=0.6)\n",
    "        axes[0, 1].set_title('Number of Detections')\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ëª¨ë‹ˆí„°\n",
    "        axes[1, 0].plot(confidence_data, 'r-', linewidth=1.5, alpha=0.7)\n",
    "        axes[1, 0].axhline(y=self.thresholds['min_confidence'], \n",
    "                          color='orange', linestyle='--', alpha=0.5,\n",
    "                          label=f'Min threshold ({self.thresholds[\"min_confidence\"]})')\n",
    "        axes[1, 0].fill_between(range(time_points), 0, 1,\n",
    "                               where=(confidence_data > 0.8),\n",
    "                               color='green', alpha=0.2, \n",
    "                               label='High confidence')\n",
    "        axes[1, 0].set_title('Average Detection Confidence')\n",
    "        axes[1, 0].set_xlabel('Time')\n",
    "        axes[1, 0].set_ylabel('Confidence')\n",
    "        axes[1, 0].set_ylim([0, 1])\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ì²˜ë¦¬ ì‹œê°„ ëª¨ë‹ˆí„°\n",
    "        axes[1, 1].plot(processing_time_data, 'm-', linewidth=1.5, alpha=0.7)\n",
    "        axes[1, 1].axhline(y=self.thresholds['max_processing_time'], \n",
    "                          color='r', linestyle='--', alpha=0.5,\n",
    "                          label=f'Max threshold ({self.thresholds[\"max_processing_time\"]} ms)')\n",
    "        axes[1, 1].set_title('Processing Time per Frame')\n",
    "        axes[1, 1].set_xlabel('Time')\n",
    "        axes[1, 1].set_ylabel('Time (ms)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # í†µê³„ ìš”ì•½\n",
    "        print(\"\\nğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìš”ì•½:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"í‰ê·  FPS: {np.mean(fps_data):.1f} (ëª©í‘œ: >{self.thresholds['min_fps']})\")\n",
    "        print(f\"í‰ê·  ê²€ì¶œ ìˆ˜: {np.mean(detections_data):.1f}\")\n",
    "        print(f\"í‰ê·  ì‹ ë¢°ë„: {np.mean(confidence_data):.3f} (ëª©í‘œ: >{self.thresholds['min_confidence']})\")\n",
    "        print(f\"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {np.mean(processing_time_data):.1f}ms (ëª©í‘œ: <{self.thresholds['max_processing_time']}ms)\")\n",
    "        \n",
    "        # ì•ŒëŒ ìƒíƒœ\n",
    "        low_fps_count = np.sum(fps_data < self.thresholds['min_fps'])\n",
    "        low_conf_count = np.sum(confidence_data < self.thresholds['min_confidence'])\n",
    "        \n",
    "        if low_fps_count > 0 or low_conf_count > 0:\n",
    "            print(\"\\nâš ï¸ ì•ŒëŒ:\")\n",
    "            if low_fps_count > 0:\n",
    "                print(f\"   - LOW FPS: {low_fps_count}íšŒ ë°œìƒ\")\n",
    "            if low_conf_count > 0:\n",
    "                print(f\"   - LOW CONFIDENCE: {low_conf_count}íšŒ ë°œìƒ\")\n",
    "        else:\n",
    "            print(\"\\nâœ… ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì—°\n",
    "print(\"ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì—°\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "monitor = RealTimeMonitor()\n",
    "monitor.create_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ Part 7: í†µí•© íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "### ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ í†µí•©í•œ ì™„ì „ ìë™í™” ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoFineTuningPipeline:\n",
    "    \"\"\"\n",
    "    ìë™ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    ì „ì²´ í”„ë¡œì„¸ìŠ¤:\n",
    "    1. ë°ì´í„° ì¤€ë¹„\n",
    "    2. í•™ìŠµ ì‹¤í–‰\n",
    "    3. ê²€ì¦ ë° í‰ê°€\n",
    "    4. ëª¨ë¸ ì„ íƒ\n",
    "    5. ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_name=\"auto_finetuning\"):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            project_name: í”„ë¡œì íŠ¸ ì´ë¦„\n",
    "        \"\"\"\n",
    "        self.project_name = project_name\n",
    "        self.dataset_preparer = None\n",
    "        self.fine_tuner = None\n",
    "        self.al_manager = None\n",
    "        self.monitor = None\n",
    "        self.results = {}\n",
    "        \n",
    "        print(f\"ğŸ¤– ìë™ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\")\n",
    "        print(f\"   í”„ë¡œì íŠ¸: {project_name}\")\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"\n",
    "        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸš€ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: ë°ì´í„° ì¤€ë¹„\n",
    "        print(\"\\n[Step 1/5] ğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„\")\n",
    "        print(\"-\" * 40)\n",
    "        self.dataset_preparer = CustomDatasetPreparer(self.project_name)\n",
    "        self.dataset_preparer.add_custom_classes(['class_a', 'class_b', 'class_c'])\n",
    "        yaml_path = self.dataset_preparer.create_yaml_config()\n",
    "        print(\"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "        time.sleep(1)  # ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´\n",
    "        \n",
    "        # Step 2: Active Learning ì„¤ì •\n",
    "        print(\"\\n[Step 2/5] ğŸ§  Active Learning ì„¤ì •\")\n",
    "        print(\"-\" * 40)\n",
    "        self.al_manager = ActiveLearningManager(uncertainty_threshold=0.3)\n",
    "        print(\"âœ… Active Learning ì„¤ì • ì™„ë£Œ\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Step 3: í•™ìŠµ ì‹¤í–‰\n",
    "        print(\"\\n[Step 3/5] ğŸ“ ëª¨ë¸ í•™ìŠµ\")\n",
    "        print(\"-\" * 40)\n",
    "        self.fine_tuner = YOLOFineTuner(base_model=\"yolo11n.pt\")\n",
    "        self.fine_tuner.configure_training(\n",
    "            epochs=100,\n",
    "            batch_size=16,\n",
    "            learning_rate=0.01\n",
    "        )\n",
    "        print(\"ğŸ’¡ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì¤‘...\")\n",
    "        \n",
    "        # í•™ìŠµ ì§„í–‰ ì‹œë®¬ë ˆì´ì…˜\n",
    "        epochs = 100\n",
    "        for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "            time.sleep(0.01)  # ì‹œë®¬ë ˆì´ì…˜\n",
    "        \n",
    "        print(\"âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "        \n",
    "        # Step 4: ëª¨ë‹ˆí„°ë§\n",
    "        print(\"\\n[Step 4/5] ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\")\n",
    "        print(\"-\" * 40)\n",
    "        self.monitor = RealTimeMonitor()\n",
    "        \n",
    "        # ì‹œë®¬ë ˆì´ì…˜ ë©”íŠ¸ë¦­\n",
    "        self.results = {\n",
    "            'training': {\n",
    "                'epochs': 100,\n",
    "                'final_loss': 0.023,\n",
    "                'training_time': '2h 15m'\n",
    "            },\n",
    "            'validation': {\n",
    "                'mAP50': 0.92,\n",
    "                'mAP50_95': 0.74,\n",
    "                'precision': 0.94,\n",
    "                'recall': 0.91\n",
    "            },\n",
    "            'performance': {\n",
    "                'inference_time': '15.3ms',\n",
    "                'fps': 65,\n",
    "                'model_size': '6.2MB'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"âœ… ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Step 5: ë¦¬í¬íŠ¸ ìƒì„±\n",
    "        print(\"\\n[Step 5/5] ğŸ“ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\")\n",
    "        print(\"-\" * 40)\n",
    "        self.generate_report()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"\n",
    "        ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\n",
    "        \"\"\"\n",
    "        report = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘              íŒŒì¸íŠœë‹ ê²°ê³¼ ë¦¬í¬íŠ¸                           â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“… ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "ğŸ“ í”„ë¡œì íŠ¸: {self.project_name}\n",
    "\n",
    "1. í•™ìŠµ ì •ë³´\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â€¢ Epochs: {self.results['training']['epochs']}\n",
    "   â€¢ Final Loss: {self.results['training']['final_loss']}\n",
    "   â€¢ Training Time: {self.results['training']['training_time']}\n",
    "\n",
    "2. ê²€ì¦ ì„±ëŠ¥\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â€¢ mAP@0.5: {self.results['validation']['mAP50']:.2%}\n",
    "   â€¢ mAP@0.5-0.95: {self.results['validation']['mAP50_95']:.2%}\n",
    "   â€¢ Precision: {self.results['validation']['precision']:.2%}\n",
    "   â€¢ Recall: {self.results['validation']['recall']:.2%}\n",
    "\n",
    "3. ì¶”ë¡  ì„±ëŠ¥\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â€¢ Inference Time: {self.results['performance']['inference_time']}\n",
    "   â€¢ FPS: {self.results['performance']['fps']}\n",
    "   â€¢ Model Size: {self.results['performance']['model_size']}\n",
    "\n",
    "4. ê°œì„  ì‚¬í•­\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â€¢ mAP í–¥ìƒ: +22.7%\n",
    "   â€¢ ì •í™•ë„ í–¥ìƒ: +14.6%\n",
    "   â€¢ ì¬í˜„ìœ¨ í–¥ìƒ: +19.7%\n",
    "\n",
    "5. ì¶”ì²œ ì‚¬í•­\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ“ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ íŒŒì¸íŠœë‹ë˜ì—ˆìŠµë‹ˆë‹¤\n",
    "   âœ“ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ\n",
    "   âœ“ ì¶”ê°€ ë°ì´í„°ë¡œ ì§€ì†ì  ê°œì„  ê°€ëŠ¥\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "        \"\"\"\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "        # íŒŒì¼ë¡œ ì €ì¥\n",
    "        report_path = f\"reports/{self.project_name}_report.txt\"\n",
    "        print(f\"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ ìœ„ì¹˜: {report_path}\")\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "print(\"\\nğŸ¤– ìë™ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹œì—°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline = AutoFineTuningPipeline(\"my_custom_detector\")\n",
    "pipeline.run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 8: ì‹¤ì „ í™œìš© ê°€ì´ë“œ\n",
    "\n",
    "### íŒŒì¸íŠœë‹ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š íŒŒì¸íŠœë‹ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "### âœ… ë°ì´í„° ì¤€ë¹„\n",
    "- [ ] ìµœì†Œ 100ì¥ ì´ìƒì˜ ì´ë¯¸ì§€\n",
    "- [ ] í´ë˜ìŠ¤ë‹¹ ê· í˜•ì¡íŒ ìƒ˜í”Œ ìˆ˜\n",
    "- [ ] ë‹¤ì–‘í•œ ê°ë„, ì¡°ëª…, ë°°ê²½\n",
    "- [ ] ì •í™•í•œ ë¼ë²¨ë§\n",
    "\n",
    "### âœ… í•™ìŠµ ì„¤ì •\n",
    "- [ ] ì ì ˆí•œ base model ì„ íƒ (n, s, m, l, x)\n",
    "- [ ] GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” batch size\n",
    "- [ ] Learning rate ì¡°ì • (0.001 ~ 0.01)\n",
    "- [ ] Early stopping ì„¤ì •\n",
    "\n",
    "### âœ… ëª¨ë‹ˆí„°ë§\n",
    "- [ ] í•™ìŠµ ê³¡ì„  í™•ì¸ (loss, mAP)\n",
    "- [ ] ê³¼ì í•© ì²´í¬\n",
    "- [ ] ê²€ì¦ ì„¸íŠ¸ ì„±ëŠ¥\n",
    "- [ ] ì¶”ë¡  ì†ë„\n",
    "\n",
    "### âœ… ìµœì í™”\n",
    "- [ ] Data augmentation ì ìš©\n",
    "- [ ] Active Learning í™œìš©\n",
    "- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "- [ ] ëª¨ë¸ ì•™ìƒë¸”\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ ì‹¤ì „ íŒ\n",
    "\n",
    "### 1. ë°ì´í„° í’ˆì§ˆ > ë°ì´í„° ì–‘\n",
    "```python\n",
    "# ì¢‹ì€ ë°ì´í„°ì˜ íŠ¹ì§•\n",
    "- ì„ ëª…í•œ ì´ë¯¸ì§€\n",
    "- ì •í™•í•œ ë°”ìš´ë”© ë°•ìŠ¤\n",
    "- ì¼ê´€ëœ ë¼ë²¨ë§\n",
    "- ì‹¤ì œ ì‚¬ìš© í™˜ê²½ê³¼ ìœ ì‚¬\n",
    "```\n",
    "\n",
    "### 2. ì ì§„ì  í•™ìŠµ\n",
    "```python\n",
    "# Step 1: ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "# Step 2: ì„±ëŠ¥ í™•ì¸ í›„ í° ëª¨ë¸ë¡œ\n",
    "if performance < target:\n",
    "    model = YOLO('yolo11m.pt')\n",
    "```\n",
    "\n",
    "### 3. ê²€ì¦ ì „ëµ\n",
    "```python\n",
    "# K-fold Cross Validation\n",
    "for fold in range(5):\n",
    "    train_data, val_data = split_data(fold)\n",
    "    model.train(train_data)\n",
    "    results.append(model.val(val_data))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ í•™ìŠµ ì™„ë£Œ!\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ YOLO11 íŒŒì¸íŠœë‹ì˜ ëª¨ë“  ê³¼ì •ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„:\n",
    "1. ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ ì‹œë„\n",
    "2. ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš©\n",
    "3. í”„ë¡œë•ì…˜ ë°°í¬\n",
    "\n",
    "### ì¶”ê°€ ìë£Œ:\n",
    "- [Ultralytics Documentation](https://docs.ultralytics.com/)\n",
    "- [GitHub Repository](https://github.com/aebonlee/YOLO11_study)\n",
    "\n",
    "ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ GitHub Issuesì— ë‚¨ê²¨ì£¼ì„¸ìš”! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ë©”ì‹œì§€\n",
    "print(\"ğŸ‰ íŒŒì¸íŠœë‹ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!\")\n",
    "print(\"\\nì´ì œ ë‹¹ì‹ ì€:\")\n",
    "print(\"âœ… ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"âœ… YOLO11 ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"âœ… Active Learningì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"âœ… ì™„ì „ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ì„ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"\\nğŸš€ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³´ì„¸ìš”!\")\n",
    "print(\"\\nGitHub: https://github.com/aebonlee/YOLO11_study\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}