{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_RMGnOgaJw0aIUwuSBlnbigcQkoiIILr","timestamp":1761643045100}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-DotPZn3nJVU"},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.optim as optim"]},{"cell_type":"markdown","source":["문제 1 (주관식 - 개념 설명)\n","- 경사 하강법(Gradient Descent)의 목표와 핵심 원리를 \"손실(Loss)\"과 \"경사(Gradient)\"라는 키워드를 사용하여 2~3줄로 간략히 설명하시오"],"metadata":{"id":"REx3fnH2nZao"}},{"cell_type":"markdown","source":["손실 함수의 값이 최소가 되는 지점을 찾기 위해 사용하는 방법이다. 역전파 방식으로 step마다 손실 함수로 경사를 계산한다.\n"],"metadata":{"id":"r0iPgNT8nrrj"}},{"cell_type":"markdown","source":["문제 2 (실습 문제 - 코드 작성)\n","\n","평균 제곱 오차(MSE: Mean Squared Error) 손실 함수를 파이토치 텐서를 사용하여 직접 구현하시오.\n","\n","- 함수 mse(Yp, Y)는 예측값 텐서 Yp와 실제값 텐서 Y를 입력받아 MSE 손실 값을 계산하여 반환해야 합니다.\n","\n","- MSE 공식: $\\text{loss} = \\frac{1}{n} \\sum_{i=1}^{n} (Yp_i - Y_i)^2$\n"],"metadata":{"id":"LpaE01Y7nehv"}},{"cell_type":"code","source":["import torch\n","\n","# 아래 함수를 완성하시오\n","def mse(Yp, Y):\n","    # Yp: 예측값 텐서, Y: 실제값 텐서\n","    loss = ((Yp - Y) ** 2).mean() # 텐서끼리 계산 후 전체 요소를 mean() 함수로 계산하여 return\n","    return loss\n","\n","# --- 테스트 코드 (수정 불필요) ---\n","Yp_test = torch.tensor([1.0, 2.5, 3.8])\n","Y_test  = torch.tensor([1.2, 2.0, 4.0])\n","# 예상 MSE = ((1.0-1.2)^2 + (2.5-2.0)^2 + (3.8-4.0)^2) / 3 = (0.04 + 0.25 + 0.04) / 3 = 0.33 / 3 = 0.11\n","test_loss = mse(Yp_test, Y_test)\n","print(f\"테스트 MSE 손실: {test_loss:.4f}\")"],"metadata":{"id":"kpDT0Xb9nWSz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761646420175,"user_tz":-540,"elapsed":42,"user":{"displayName":"김다봄","userId":"02209919459503367304"}},"outputId":"cd7b9ee6-00e5-4e9a-e22a-4cd7f31870ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["테스트 MSE 손실: 0.1100\n"]}]},{"cell_type":"markdown","source":["문제 3 (실습 문제 - 코드 빈칸 채우기)\n","\n","아래 코드의 빈칸 ( # TODO: ... 부분)을 채워 파라미터 W와 B를 업데이트하는 과정을 완성하시오.\n","\n","요구사항\n","\n","1. loss.backward()를 호출하여 경사를 계산합니다.\n","2. torch.no_grad() 컨텍스트 내에서 W와 B를 학습률(lr)과 계산된 경사(.grad)를 이용하여 업데이트합니다.\n","3. 다음 반복을 위해 W와 B의 경사 값을 0으로 초기화합니다 (.grad.zero_()).\n","\n","- 참고 : first_ml.ipynb)의 경사 하강법 반복 학습 부분을 참고"],"metadata":{"id":"iMr1g2wToDrc"}},{"cell_type":"code","source":["import torch\n","import numpy as np # 예시 데이터 생성을 위해 사용\n","\n","# 예시 데이터 (수정 불필요)\n","X = torch.tensor([-5.,  5.,  0.,  2., -2.]).float()\n","Y = torch.tensor([-6.7, 10.3, -3.3, 5.0, -5.3]).float()\n","\n","# 초기 파라미터 및 학습률 (수정 불필요)\n","W = torch.tensor(1.0, requires_grad=True).float()\n","B = torch.tensor(1.0, requires_grad=True).float()\n","lr = 0.001\n","\n","# 예측 함수 및 손실 함수 (수정 불필요)\n","def pred(X): return W * X + B\n","def mse(Yp, Y): return ((Yp - Y)**2).mean()\n","\n","# --- 1회 반복 학습 과정 ---\n","# 예측 계산 (수정 불필요)\n","Yp = pred(X)\n","\n","# 손실 계산 (수정 불필요)\n","loss = mse(Yp, Y) # 예측값, 정답값이 맞는 비율 계산\n","\n","# TODO: 1. 경사 계산\n","# 이 pass를 지우고 코드를 작성하세요.\n","loss.backward()\n","\n","# 경사 업데이트 (torch.no_grad() 사용)\n","with torch.no_grad():\n","    # TODO: 2. W 파라미터 업데이트\n","    W -= lr * W.grad\n","    # TODO: 3. B 파라미터 업데이트\n","    B -= lr * B.grad\n","\n","# TODO: 4. W와 B의 경사 초기화\n","W.grad.zero_()\n","B.grad.zero_()\n","\n","# --- 결과 확인 (수정 불필요) ---\n","print(f\"업데이트 후 W: {W.item():.4f}\") # 초기 W=1.0, 초기 loss=13.3520, W.grad=-19.04, lr=0.001 -> 1.0 - 0.001*(-19.04) = 1.01904\n","print(f\"업데이트 후 B: {B.item():.4f}\") # 초기 B=1.0, B.grad=2.0, lr=0.001 -> 1.0 - 0.001*(2.0) = 0.998\n","print(f\"W의 현재 경사: {W.grad}\") # 초기화 후에는 0 또는 None 이어야 함\n","print(f\"B의 현재 경사: {B.grad}\") # 초기화 후에는 0 또는 None 이어야 함"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8oWowWM-oOSD","executionInfo":{"status":"ok","timestamp":1761646783861,"user_tz":-540,"elapsed":11,"user":{"displayName":"김다봄","userId":"02209919459503367304"}},"outputId":"deb7160b-1921-493c-c889-95b4869962af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["업데이트 후 W: 1.0190\n","업데이트 후 B: 0.9980\n","W의 현재 경사: 0.0\n","B의 현재 경사: 0.0\n"]}]},{"cell_type":"markdown","source":["문제 4 (실습 문제 - 코드 빈칸 채우기)\n","- torch.optim 라이브러리를 사용하여 확률적 경사 하강법(SGD) 옵티마이저를 생성하고, 이를 이용해 파라미터를 업데이트하는 코드의 빈칸을 채우시오\n","\n","요구사항\n","1. optim.SGD를 사용하여 W와 B를 업데이트하는 옵티마이저(optimizer)를 생성합니다. 학습률(lr)도 지정해야 합니다.\n","2. 계산된 경사를 이용하여 파라미터를 업데이트하기 위해 optimizer.step()를 호출합니다.\n","3. 다음 반복을 위해 옵티마이저에 연결된 파라미터들의 경사도를 0으로 초기화하기 위해 optimizer.zero_grad()를 호출합니다."],"metadata":{"id":"9FoboM3hpPcd"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import numpy as np # 예시 데이터 생성을 위해 사용\n","\n","# 예시 데이터 (수정 불필요)\n","X = torch.tensor([-5.,  5.,  0.,  2., -2.]).float()\n","Y = torch.tensor([-6.7, 10.3, -3.3, 5.0, -5.3]).float()\n","\n","# 초기 파라미터 및 학습률 (수정 불필요)\n","W = torch.tensor(1.0, requires_grad=True).float()\n","B = torch.tensor(1.0, requires_grad=True).float()\n","lr = 0.001\n","\n","# 예측 함수 및 손실 함수 (수정 불필요)\n","def pred(X): return W * X + B\n","def mse(Yp, Y): return ((Yp - Y)**2).mean()\n","\n","# TODO: 1. SGD 옵티마이저 생성 (파라미터: [W, B], 학습률: lr)\n","optimizer = optim.SGD([W, B], lr=lr) # 이 None을 지우고 코드를 작성하세요.\n","\n","# --- 1회 반복 학습 과정 ---\n","# 예측 계산 (수정 불필요)\n","Yp = pred(X)\n","\n","# 손실 계산 (수정 불필요)\n","loss = mse(Yp, Y)\n","\n","# 경사 계산 (수정 불필요)\n","loss.backward()\n","\n","# TODO: 2. 옵티마이저를 이용한 파라미터 업데이트\n","optimizer.step()\n","\n","# TODO: 3. 옵티마이저 경사 초기화\n","optimizer.zero_grad()\n","\n","\n","# --- 결과 확인 (수정 불필요) ---\n","# 결과는 문제 3과 동일해야 함\n","print(f\"옵티마이저 사용 후 W: {W.item():.4f}\")\n","print(f\"옵티마이저 사용 후 B: {B.item():.4f}\")\n","print(f\"W의 현재 경사: {W.grad}\")\n","print(f\"B의 현재 경사: {B.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7_68yY4n_ee","executionInfo":{"status":"ok","timestamp":1761646843292,"user_tz":-540,"elapsed":7537,"user":{"displayName":"김다봄","userId":"02209919459503367304"}},"outputId":"ea78d3a5-f5b8-4b19-a1cd-a545bba3085c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["옵티마이저 사용 후 W: 1.0190\n","옵티마이저 사용 후 B: 0.9980\n","W의 현재 경사: None\n","B의 현재 경사: None\n"]}]},{"cell_type":"markdown","source":["문제 5(주관식-개념설명)\n","\n","모델 학습 시 발생할 수 있는 과소적합(Underfitting)과 과대적합(Overfitting)의 특징을 편향(Bias)과 분산(Variance) 관점에서 각각 설명하시오. (각각 1~2줄)"],"metadata":{"id":"ENK7dByipgd4"}},{"cell_type":"markdown","source":["모델 학습 시 데이터가 너무 적으면 모델 예측에 편향Bias가 높아져 단순하게 예측값이 한쪽으로 쏠리게 되는 과소적합Underfitting이 발생하게 되고, 반대로 학습 데이터가 너무 과다할 경우 많은 데이터에 많은 가중치를 두어 분산Variance가 높아져 과대적합Overfitting이 발생한다.\n"],"metadata":{"id":"i8Rw3ACHprxU"}},{"cell_type":"markdown","source":["문제 6 (주관식 - 평가 지표 선택)\n","- 다음과 같은 두 가지 상황에 가장 적합한 분류 모델 평가 지표를 각각 선택하고 그 이유를 간략히 설명하시오.\n","\n","- 상황 1: 환자의 의료 데이터를 분석하여 암 진단 여부를 예측하는 모델 (실제 암 환자를 놓치면 안 되는 경우, 즉 FN(False Negative)을 최소화해야 함)\n","\n","- 상황 2: 이메일 데이터를 분석하여 스팸 메일 여부를 필터링하는 모델 (정상 메일을 스팸으로 잘못 분류하면 안 되는 경우, 즉 FP(False Positive)를 최소화해야 함)\n","\n","- 선택 가능한 지표: 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-Score"],"metadata":{"id":"JE9Iio5_rqGC"}},{"cell_type":"markdown","source":["상황 1에서는 실제 암 환자Negative로 예측한 데이터가 암 환자가 아닌 False인 FN 비율을 줄여야하므로 재현율이 높아야 하고,\n","상황 2에서는 정상 메일 Positive로 예측한 데이터가 스팸 False인 FP 비율을 줄여야하므로 정밀도가 높아야 한다."],"metadata":{"id":"2eqQ4XdWr1ol"}},{"cell_type":"markdown","source":["문제 6.\n","실전 프로젝트 라이브 코딩 하세요."],"metadata":{"id":"5POZQfWaPR6o"}},{"cell_type":"code","source":[],"metadata":{"id":"6btrts6pPUg9"},"execution_count":null,"outputs":[]}]}