{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1w7kldlBdCaDytQS7ASwg_HL-CWVcQPUr","timestamp":1761807062834}],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxS7OqMPkDgI","executionInfo":{"status":"ok","timestamp":1761808400482,"user_tz":-540,"elapsed":46058,"user":{"displayName":"김다봄","userId":"02209919459503367304"}},"outputId":"c6458af5-7218-4875-81f2-d9401f908928"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","Part 5: BatchNorm과 Gradient 안정성\n","======================================================================\n","\n","[실습 5-1] Batch Size가 학습에 미치는 영향\n","\n","데이터셋 생성 중...\n","훈련 데이터: (800, 20)\n","\n","Batch Size 8 학습 중...\n","  최종 손실: 0.0022\n","\n","Batch Size 32 학습 중...\n","  최종 손실: 0.0223\n","\n","Batch Size 128 학습 중...\n","  최종 손실: 0.1249\n","\n","Batch Size 512 학습 중...\n","  최종 손실: 0.3510\n","\n","저장: part5_batch_size_effect.png\n","\n","======================================================================\n","[실습 5-2] BatchNorm의 효과 비교\n","======================================================================\n","\n","BatchNorm 원리:\n","  1. 각 배치의 평균과 분산 계산\n","  2. 정규화: (x - 평균) / sqrt(분산 + epsilon)\n","  3. 스케일 및 이동: gamma * 정규화값 + beta\n","  4. gamma, beta는 학습 가능한 파라미터\n","\n","Without BatchNorm 학습 중...\n","  Epoch 10/50 - Loss: 0.0432\n","  Epoch 20/50 - Loss: 0.0022\n","  Epoch 30/50 - Loss: 0.0006\n","  Epoch 40/50 - Loss: 0.0003\n","  Epoch 50/50 - Loss: 0.0001\n","\n","With BatchNorm 학습 중...\n","  Epoch 10/50 - Loss: 0.0993\n","  Epoch 20/50 - Loss: 0.0313\n","  Epoch 30/50 - Loss: 0.0231\n","  Epoch 40/50 - Loss: 0.0127\n","  Epoch 50/50 - Loss: 0.0184\n","\n","저장: part5_batchnorm_effect.png\n","\n","======================================================================\n","[실습 5-3] BatchNorm의 Gradient 안정화 효과\n","======================================================================\n","\n","Hook 등록 중...\n","\n","Forward/Backward 수행 중...\n","\n","각 층의 그래디언트 크기:\n","\n","Without BatchNorm:\n","  Layer 1: 0.000269\n","  Layer 2: 0.000241\n","  Layer 3: 0.000208\n","  Layer 4: 0.000408\n","  Layer 5: 0.004996\n","\n","With BatchNorm:\n","  Layer 1: 0.008942\n","  Layer 2: 0.004626\n","  Layer 3: 0.007234\n","  Layer 4: 0.003369\n","  Layer 5: 0.006100\n","  Layer 6: 0.002800\n","  Layer 7: 0.006016\n","  Layer 8: 0.004889\n","  Layer 9: 0.066056\n","\n","저장: part5_gradient_stability.png\n","\n","분석:\n","  - BatchNorm 없음: 층마다 그래디언트 크기 차이가 큼\n","  - BatchNorm 있음: 모든 층에서 그래디언트가 비슷한 크기\n","  - 결과: 더 안정적인 학습 가능\n","\n","======================================================================\n","[실습 5-4] 다양한 Batch Size에서 BatchNorm 효과\n","======================================================================\n","\n","Batch Size 8 테스트 중...\n","  Without BN 최종 손실: 0.0000\n","  With BN 최종 손실: 0.1877\n","\n","Batch Size 16 테스트 중...\n","  Without BN 최종 손실: 0.0003\n","  With BN 최종 손실: 0.0857\n","\n","Batch Size 32 테스트 중...\n","  Without BN 최종 손실: 0.0006\n","  With BN 최종 손실: 0.0150\n","\n","Batch Size 64 테스트 중...\n","  Without BN 최종 손실: 0.0014\n","  With BN 최종 손실: 0.0133\n","\n","저장: part5_batchnorm_various_batch_sizes.png\n","\n","======================================================================\n","[실습 5-5] BatchNorm의 학습/평가 모드 차이\n","======================================================================\n","\n","모델 학습 중...\n","학습 완료\n","\n","학습 모드 vs 평가 모드 출력 비교:\n","\n","학습 모드 예측 분산: 0.00000000\n","평가 모드 예측 분산: 0.00000000\n","\n","설명:\n","  - 학습 모드: 각 배치의 통계 사용 -> 예측이 약간 다름\n","  - 평가 모드: 전체 데이터의 Moving Average 사용 -> 일관된 예측\n","  - 중요: model.eval()을 반드시 사용해야 함!\n","\n","======================================================================\n","Part 5 완료\n","======================================================================\n","\n","핵심 개념:\n","\n","1. Batch Size의 영향\n","   - 작은 배치: 노이즈 많음, 정규화 효과\n","   - 큰 배치: 안정적, GPU 활용 좋음\n","   - 권장: 32-128 사이\n","\n","2. Batch Normalization\n","   - 목적: 각 층의 입력 분포 안정화\n","   - 효과: 빠른 학습, 높은 학습률 가능\n","   - 위치: Linear/Conv -> BN -> Activation\n","\n","3. Gradient 안정화\n","   - BN 없음: 층마다 그래디언트 크기 차이\n","   - BN 있음: 모든 층에서 균일한 그래디언트\n","   - 결과: 깊은 네트워크 학습 가능\n","\n","4. 학습/평가 모드\n","   - 학습: 현재 배치 통계 사용\n","   - 평가: Moving Average 사용\n","   - 필수: model.train() / model.eval() 구분\n","\n","생성된 파일:\n","  1. part5_batch_size_effect.png - Batch Size 영향\n","  2. part5_batchnorm_effect.png - BatchNorm 효과\n","  3. part5_gradient_stability.png - Gradient 안정성\n","  4. part5_batchnorm_various_batch_sizes.png - 다양한 배치에서 BN 효과\n","\n","5차시 실습 완료!\n","======================================================================\n"]}],"source":["\"\"\"\n","Part 5: BatchNorm과 Gradient 안정성\n","- Minibatch 크기의 영향\n","- Batch Normalization의 효과\n","- Gradient 안정성 비교\n","\n","독립적으로 실행 가능합니다.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# 재현성을 위한 시드 고정\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","print(\"=\" * 70)\n","print(\"Part 5: BatchNorm과 Gradient 안정성\")\n","print(\"=\" * 70)\n","\n","\n","# =====================================================================\n","# 실습 5-1: Batch Size의 영향 관찰\n","# =====================================================================\n","print(\"\\n[실습 5-1] Batch Size가 학습에 미치는 영향\")\n","\n","class SimpleNet(nn.Module):\n","    \"\"\"BatchNorm 없는 기본 신경망\"\"\"\n","    def __init__(self):\n","        super(SimpleNet, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(20, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1), # 이진분류가 된다 #??\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","# 데이터셋 준비(독립 종속변수)\n","print(\"\\n데이터셋 생성 중...\")\n","X, y = make_classification(\n","    n_samples=1000, n_features=20, n_informative=15,\n","    random_state=42\n",")\n","# 여기부분은 다 외우기. statify=y ; 잘섞이게 층마다 분류하겠다\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# 표준화 스케일링 (y는 표준화 안함)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","X_train_t = torch.FloatTensor(X_train)\n","y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n","\n","'''\n","# 연습\n","X_train, X_test, y_train, y_test = train_test_split(\n","  X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","X_train_t = torch.FloatTensor(X_train)\n","y_train_t = torch.FlaotTensor(y_train).unsqueeze(1)\n","'''\n","print(f\"훈련 데이터: {X_train.shape}\")\n","\n","# 다양한 Batch Size로 학습 - Minibatch 크기의 영향\n","batch_sizes = [8, 32, 128, 512]\n","criterion = nn.BCELoss()\n","\n","batch_size_results = []\n","\n","for batch_size in batch_sizes:\n","    print(f\"\\nBatch Size {batch_size} 학습 중...\")\n","\n","    # DataLoader 생성\n","    dataset = TensorDataset(X_train_t, y_train_t) # 스케일링된 데이터 넣고\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # 데이터로드에 집어넣기. 한번 돌때마다 셔플(랜덤)\n","\n","    # 모델 초기화\n","    model = SimpleNet()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    # 학습\n","    losses = []\n","    num_epochs = 30\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        for batch_X, batch_y in dataloader:\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            output = model(batch_X) # 기존에 통으로 넣었던걸 배치단위로 넣는거.\n","            loss = criterion(output, batch_y)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item() # loss값 숫자로 바꿔서 넣기\n","\n","        avg_loss = epoch_loss / len(dataloader)\n","        losses.append(avg_loss)\n","\n","    batch_size_results.append((batch_size, losses))\n","    print(f\"  최종 손실: {losses[-1]:.4f}\")\n","\n","# 시각화\n","plt.figure(figsize=(12, 5)) # 도화지 생성\n","\n","plt.subplot(1, 2, 1) # (2,1)짜리 sub plot 만듦\n","for batch_size, losses in batch_size_results:\n","    plt.plot(losses, label=f'Batch={batch_size}', linewidth=2)\n","\n","plt.xlabel('Epoch', fontsize=12)\n","plt.ylabel('Training Loss', fontsize=12)\n","plt.title('Effect of Batch Size', fontsize=14, weight='bold')\n","plt.legend(fontsize=10) # 범례\n","plt.grid(alpha=0.3) # 격자무늬\n","\n","plt.subplot(1, 2, 2)\n","# loss값 중 가장 마지막 loss값만 보겠다\n","final_losses = [losses[-1] for _, losses in batch_size_results]\n","colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n","bars = plt.bar([str(bs) for bs in batch_sizes], final_losses,\n","               color=colors, edgecolor='black', alpha=0.7) # alpha: 투명도\n","\n","plt.xlabel('Batch Size', fontsize=12)\n","plt.ylabel('Final Loss', fontsize=12)\n","plt.title('Final Loss by Batch Size', fontsize=14, weight='bold')\n","plt.grid(axis='y', alpha=0.3)\n","\n","# 값 표시\n","for bar, loss in zip(bars, final_losses): # enumarate, zip 많이 씀 # zip은 튜플 형태로 갖고 온다.\n","    height = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2., height,\n","             f'{loss:.4f}', ha='center', va='bottom', fontsize=10)\n","\n","plt.tight_layout()\n","plt.savefig('part5_batch_size_effect.png', dpi=150, bbox_inches='tight')\n","print(\"\\n저장: part5_batch_size_effect.png\")\n","plt.close()\n","\n","\n","# =====================================================================\n","# 실습 5-2: BatchNorm 없는 네트워크 vs BatchNorm 있는 네트워크\n","# =====================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"[실습 5-2] BatchNorm의 효과 비교\")\n","print(\"=\" * 70)\n","\n","class NetWithoutBN(nn.Module):\n","    \"\"\"BatchNorm 없는 깊은 신경망\"\"\"\n","    def __init__(self):\n","        super(NetWithoutBN, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(20, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","class NetWithBN(nn.Module):\n","    \"\"\"BatchNorm 있는 깊은 신경망\"\"\"\n","    def __init__(self):\n","        super(NetWithBN, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(20, 64),\n","            nn.BatchNorm1d(64),  # BatchNorm 추가\n","            nn.ReLU(),\n","            nn.Linear(64, 64), # ??\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64), # ??\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","print(\"\\nBatchNorm 원리:\")\n","print(\"  1. 각 배치의 평균과 분산 계산\")\n","print(\"  2. 정규화: (x - 평균) / sqrt(분산 + epsilon)\")\n","print(\"  3. 스케일 및 이동: gamma * 정규화값 + beta\")\n","print(\"  4. gamma, beta는 학습 가능한 파라미터\")\n","\n","# 두 모델 학습\n","models_to_compare = [\n","    ('Without BatchNorm', NetWithoutBN()),\n","    ('With BatchNorm', NetWithBN())\n","]\n","\n","comparison_results = []\n","\n","# DataLoader 생성 (Batch Size 32)\n","dataset = TensorDataset(X_train_t, y_train_t)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True) # 아까 전 했던거에서 batch_size=8에서 가장 성능 좋았음\n","\n","for model_name, model in models_to_compare:\n","    print(f\"\\n{model_name} 학습 중...\")\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    losses = []\n","    num_epochs = 50\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        for batch_X, batch_y in dataloader:\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            output = model(batch_X)\n","            loss = criterion(output, batch_y)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(dataloader)\n","        losses.append(avg_loss)\n","\n","        if (epoch + 1) % 10 == 0:\n","          # .4f : 소숫점 넷째자리까지\n","            print(f\"  Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n","\n","    comparison_results.append((model_name, losses))\n","\n","# 시각화\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","for model_name, losses in comparison_results:\n","    plt.plot(losses, label=model_name, linewidth=2)\n","\n","plt.xlabel('Epoch', fontsize=12)\n","plt.ylabel('Training Loss', fontsize=12)\n","plt.title('BatchNorm Effect on Training', fontsize=14, weight='bold')\n","plt.legend(fontsize=11)\n","plt.grid(alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","for model_name, losses in comparison_results:\n","    plt.plot(losses, label=model_name, linewidth=2)\n","\n","plt.xlabel('Epoch', fontsize=12)\n","plt.ylabel('Training Loss (log scale)', fontsize=12)\n","plt.title('BatchNorm Effect (Log Scale)', fontsize=14, weight='bold')\n","plt.yscale('log')\n","plt.legend(fontsize=11)\n","plt.grid(alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('part5_batchnorm_effect.png', dpi=150, bbox_inches='tight')\n","print(\"\\n저장: part5_batchnorm_effect.png\")\n","plt.close()\n","\n","\n","# =====================================================================\n","# 실습 5-3: BatchNorm의 Gradient 안정화 효과\n","# =====================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"[실습 5-3] BatchNorm의 Gradient 안정화 효과\")\n","print(\"=\" * 70)\n","\n","# 그래디언트 수집\n","gradient_history_without_bn = []\n","gradient_history_with_bn = []\n","\n","# 새 모델 생성\n","model_without_bn = NetWithoutBN()\n","model_with_bn = NetWithBN()\n","\n","# 그래디언트 기록 Hook\n","def create_gradient_hook(gradient_list):\n","    \"\"\"그래디언트를 리스트에 저장하는 Hook 생성\"\"\"\n","    def hook(grad):\n","        gradient_list.append(grad.abs().mean().item())\n","        return grad\n","    return hook\n","\n","# Hook 등록\n","print(\"\\nHook 등록 중...\")\n","\n","# Without BN\n","for name, param in model_without_bn.named_parameters():\n","    if 'weight' in name: # 이름이 weight면 hook에 등록해라\n","        param.register_hook(create_gradient_hook(gradient_history_without_bn))\n","\n","# With BN\n","for name, param in model_with_bn.named_parameters():\n","    if 'weight' in name:\n","        param.register_hook(create_gradient_hook(gradient_history_with_bn))\n","\n","# 한 번의 Forward/Backward\n","print(\"\\nForward/Backward 수행 중...\")\n","\n","batch_X = X_train_t[:32]  # 배치 크기 32\n","batch_y = y_train_t[:32]\n","\n","# Without BN\n","output_without = model_without_bn(batch_X)\n","loss_without = criterion(output_without, batch_y)\n","loss_without.backward()\n","\n","# With BN\n","output_with = model_with_bn(batch_X)\n","loss_with = criterion(output_with, batch_y)\n","loss_with.backward()\n","\n","# 그래디언트 역순 정렬 (Layer 1부터 표시) reverse()\n","gradient_history_without_bn.reverse()\n","gradient_history_with_bn.reverse()\n","\n","print(\"\\n각 층의 그래디언트 크기:\")\n","print(\"\\nWithout BatchNorm:\")\n","for i, grad in enumerate(gradient_history_without_bn): # enumerate : 인덱스도 전달\n","    print(f\"  Layer {i+1}: {grad:.6f}\")\n","\n","print(\"\\nWith BatchNorm:\")\n","for i, grad in enumerate(gradient_history_with_bn):\n","    print(f\"  Layer {i+1}: {grad:.6f}\")\n","\n","# 시각화\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Without BN\n","ax1 = axes[0] # 1부터 시작하니까 n+1까지 돌려서 layers에 list로 넣어줌\n","layers = list(range(1, len(gradient_history_without_bn) + 1))\n","ax1.bar(layers, gradient_history_without_bn, color='#e74c3c',\n","        edgecolor='black', alpha=0.7)\n","ax1.set_xlabel('Layer Number', fontsize=11)\n","ax1.set_ylabel('Gradient Magnitude', fontsize=11)\n","ax1.set_title('Without BatchNorm', fontsize=12, weight='bold')\n","ax1.set_xticks(layers)\n","ax1.grid(axis='y', alpha=0.3)\n","\n","# With BN\n","ax2 = axes[1]\n","layers = list(range(1, len(gradient_history_with_bn) + 1))\n","ax2.bar(layers, gradient_history_with_bn, color='#2ecc71',\n","        edgecolor='black', alpha=0.7)\n","ax2.set_xlabel('Layer Number', fontsize=11)\n","ax2.set_ylabel('Gradient Magnitude', fontsize=11)\n","ax2.set_title('With BatchNorm', fontsize=12, weight='bold')\n","ax2.set_xticks(layers)\n","ax2.grid(axis='y', alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('part5_gradient_stability.png', dpi=150, bbox_inches='tight')\n","print(\"\\n저장: part5_gradient_stability.png\")\n","plt.close()\n","\n","print(\"\\n분석:\")\n","print(\"  - BatchNorm 없음: 층마다 그래디언트 크기 차이가 큼\")\n","print(\"  - BatchNorm 있음: 모든 층에서 그래디언트가 비슷한 크기\")\n","print(\"  - 결과: 더 안정적인 학습 가능\")\n","\n","\n","# =====================================================================\n","# 실습 5-4: 다양한 Batch Size에서 BatchNorm 효과\n","# =====================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"[실습 5-4] 다양한 Batch Size에서 BatchNorm 효과\")\n","print(\"=\" * 70)\n","\n","batch_sizes_test = [8, 16, 32, 64]\n","\n","results_comparison = {} # 딕셔너리 형태\n","\n","for batch_size in batch_sizes_test:\n","    print(f\"\\nBatch Size {batch_size} 테스트 중...\")\n","\n","    dataset = TensorDataset(X_train_t, y_train_t)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    # Without BN\n","    model_no_bn = NetWithoutBN()\n","    optimizer_no_bn = optim.Adam(model_no_bn.parameters(), lr=0.001)\n","\n","    losses_no_bn = []\n","    for epoch in range(30):\n","        epoch_loss = 0\n","        for batch_X, batch_y in dataloader:\n","            model_no_bn.train()\n","            optimizer_no_bn.zero_grad()\n","            output = model_no_bn(batch_X)\n","            loss = criterion(output, batch_y)\n","            loss.backward()\n","            optimizer_no_bn.step()\n","            epoch_loss += loss.item()\n","        losses_no_bn.append(epoch_loss / len(dataloader))\n","\n","    # With BN\n","    model_with_bn = NetWithBN()\n","    optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.001)\n","\n","    losses_with_bn = []\n","    for epoch in range(30):\n","        epoch_loss = 0\n","        for batch_X, batch_y in dataloader:\n","            model_with_bn.train()\n","            optimizer_with_bn.zero_grad()\n","            output = model_with_bn(batch_X)\n","            loss = criterion(output, batch_y)\n","            loss.backward()\n","            optimizer_with_bn.step()\n","            epoch_loss += loss.item()\n","        losses_with_bn.append(epoch_loss / len(dataloader))\n","\n","    results_comparison[batch_size] = {\n","        'without_bn': losses_no_bn,\n","        'with_bn': losses_with_bn\n","    }\n","\n","    print(f\"  Without BN 최종 손실: {losses_no_bn[-1]:.4f}\")\n","    print(f\"  With BN 최종 손실: {losses_with_bn[-1]:.4f}\")\n","\n","# 시각화\n","fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","\n","for idx, batch_size in enumerate(batch_sizes_test):\n","    ax = axes[idx // 2, idx % 2] # 홀수에 뭐지 이게 # ??\n","\n","    losses_no_bn = results_comparison[batch_size]['without_bn']\n","    losses_with_bn = results_comparison[batch_size]['with_bn']\n","\n","    ax.plot(losses_no_bn, label='Without BN', linewidth=2, color='#e74c3c')\n","    ax.plot(losses_with_bn, label='With BN', linewidth=2, color='#2ecc71')\n","\n","    ax.set_xlabel('Epoch', fontsize=11)\n","    ax.set_ylabel('Training Loss', fontsize=11)\n","    ax.set_title(f'Batch Size = {batch_size}', fontsize=12, weight='bold')\n","    ax.legend(fontsize=10)\n","    ax.grid(alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('part5_batchnorm_various_batch_sizes.png', dpi=150, bbox_inches='tight')\n","print(\"\\n저장: part5_batchnorm_various_batch_sizes.png\")\n","plt.close()\n","\n","\n","# =====================================================================\n","# 실습 5-5: BatchNorm 학습 모드 vs 평가 모드\n","# =====================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"[실습 5-5] BatchNorm의 학습/평가 모드 차이\")\n","print(\"=\" * 70)\n","\n","# BatchNorm이 있는 모델 생성 및 학습\n","model_bn = NetWithBN()\n","optimizer = optim.Adam(model_bn.parameters(), lr=0.001)\n","\n","dataset = TensorDataset(X_train_t, y_train_t)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","print(\"\\n모델 학습 중...\")\n","for epoch in range(30):\n","    for batch_X, batch_y in dataloader:\n","        model_bn.train()  # 학습 모드\n","        optimizer.zero_grad()\n","        output = model_bn(batch_X)\n","        loss = criterion(output, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","\n","print(\"학습 완료\")\n","\n","# 테스트 데이터\n","X_test_t = torch.FloatTensor(X_test)\n","\n","print(\"\\n학습 모드 vs 평가 모드 출력 비교:\")\n","\n","# 학습 모드에서 여러 번 예측\n","model_bn.train()\n","predictions_train_mode = []\n","for _ in range(5):\n","    with torch.no_grad():\n","        pred = model_bn(X_test_t[:10])\n","        predictions_train_mode.append(pred.numpy()) # 벡터형태로 만들기\n","\n","# 평가 모드에서 여러 번 예측\n","model_bn.eval()\n","predictions_eval_mode = []\n","for _ in range(5):\n","    with torch.no_grad():\n","        pred = model_bn(X_test_t[:10])\n","        predictions_eval_mode.append(pred.numpy())\n","\n","# 예측 분산 계산\n","variance_train = np.var(predictions_train_mode, axis=0).mean()\n","variance_eval = np.var(predictions_eval_mode, axis=0).mean()\n","\n","print(f\"\\n학습 모드 예측 분산: {variance_train:.8f}\")\n","print(f\"평가 모드 예측 분산: {variance_eval:.8f}\")\n","\n","print(\"\\n설명:\")\n","print(\"  - 학습 모드: 각 배치의 통계 사용 -> 예측이 약간 다름\")\n","print(\"  - 평가 모드: 전체 데이터의 Moving Average 사용 -> 일관된 예측\")\n","print(\"  - 중요: model.eval()을 반드시 사용해야 함!\")\n","\n","\n","# =====================================================================\n","# 최종 요약\n","# =====================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"Part 5 완료\")\n","print(\"=\" * 70)\n","\n","print(\"\\n핵심 개념:\")\n","print(\"\\n1. Batch Size의 영향\")\n","print(\"   - 작은 배치: 노이즈 많음, 정규화 효과\")\n","print(\"   - 큰 배치: 안정적, GPU 활용 좋음\")\n","print(\"   - 권장: 32-128 사이\")\n","\n","print(\"\\n2. Batch Normalization\")\n","print(\"   - 목적: 각 층의 입력 분포 안정화\")\n","print(\"   - 효과: 빠른 학습, 높은 학습률 가능\")\n","print(\"   - 위치: Linear/Conv -> BN -> Activation\")\n","\n","print(\"\\n3. Gradient 안정화\")\n","print(\"   - BN 없음: 층마다 그래디언트 크기 차이\")\n","print(\"   - BN 있음: 모든 층에서 균일한 그래디언트\")\n","print(\"   - 결과: 깊은 네트워크 학습 가능\")\n","\n","print(\"\\n4. 학습/평가 모드\")\n","print(\"   - 학습: 현재 배치 통계 사용\")\n","print(\"   - 평가: Moving Average 사용\")\n","print(\"   - 필수: model.train() / model.eval() 구분\")\n","\n","print(\"\\n생성된 파일:\")\n","print(\"  1. part5_batch_size_effect.png - Batch Size 영향\")\n","print(\"  2. part5_batchnorm_effect.png - BatchNorm 효과\")\n","print(\"  3. part5_gradient_stability.png - Gradient 안정성\")\n","print(\"  4. part5_batchnorm_various_batch_sizes.png - 다양한 배치에서 BN 효과\")\n","\n","print(\"\\n5차시 실습 완료!\")\n","print(\"=\" * 70)"]}]}