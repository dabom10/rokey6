{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19_jWvZZeYABP_VoJYq5i-mt8GflXjV0u","timestamp":1761800024042}],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7w959fkTVvbH","executionInfo":{"status":"ok","timestamp":1761803109579,"user_tz":-540,"elapsed":64557,"user":{"displayName":"김다봄","userId":"02209919459503367304"}},"outputId":"0ea4bc04-e673-4a76-e484-480b346e77d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Acc: MSE=0.453 vs BCEWithLogits=0.441\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:02<00:00, 4.75MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 125kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.18MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 10.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["MNIST Acc: LabelSmoothing=0.989 | WeightedCE=0.986\n"]}],"source":["# 6차시 보강 실습: 손실함수 비교 (Colab 호환)\n","import torch, torch.nn as nn, torch.optim as optim\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader # 디렉토리(에 있는 모든 파일)를 통채로 옮긴다\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # cuda 쓸수있음 쓰고 아니면 cpu\n","torch.manual_seed(0) # 왜 0이지\n","\n","# (A) Binary: MSE vs BCEWithLogits\n","# 데이터 생성 및 텐서 전환(차원 늘리면서)\n","X, y = make_classification(n_samples=4000, n_features=20, n_informative=8, weights=[0.6, 0.4], random_state=0)\n","Xtr, Xte, ytr, yte = train_test_split(torch.tensor(X, dtype=torch.float32),\n","                                      torch.tensor(y, dtype=torch.float32).unsqueeze(1), test_size=0.3, random_state=0)\n","\n","# binary network (nn.Module : 아 상속받는구나)\n","class BinNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.m = nn.Sequential(nn.Linear(20,64), nn.ReLU(), nn.Linear(64,1))\n","    def forward(self,x): return self.m(x)\n","\n","def run_binary(loss_fn):\n","    net = BinNet().to(device) # gpu로 넣어줘라(모델 돌려라)\n","    opt = optim.AdamW(net.parameters(), lr=1e-3) # 처음엔 학습 빨리하고 중간엔 느리게하는걸 내장한 adam, lr =0.0001\n","    net.train(); opt.zero_grad() # 초기화\n","    out = net(Xtr.to(device)) # 훈련(gpu에서 학습해줘)\n","    loss = loss_fn(out, ytr.to(device)) # 손실 계산\n","    loss.backward(); opt.step() # 역전파하고 파라미터 업데이트\n","    with torch.no_grad(): # 경사계산=자동미분중지=학습중단, 평가 시작\n","        p = torch.sigmoid(net(Xte.to(device))) # 출력층으로 보낸것\n","        acc = ((p>0.5).float()==yte.to(device)).float().mean().item()\n","    return acc\n","\n","# lambda 익명함수(x를 X로 치환하는 것 같은.. 계산하기 위해 잠깐의 과정?)\n","acc_mse = run_binary(lambda out, y: nn.MSELoss()(torch.sigmoid(out), y))\n","acc_bce = run_binary(nn.BCEWithLogitsLoss())\n","print(f\"Binary Acc: MSE={acc_mse:.3f} vs BCEWithLogits={acc_bce:.3f}\")\n","\n","# (B) Multiclass: Label Smoothing & Class Weights\n","tfm = transforms.Compose([transforms.ToTensor()]) # compose=작성하다., 전처리 툴(그냥 갖고오는 게 아니라 ex, float(stirng-value) 계산을 위해 float 전처리.이런 것처럼 tensor로 전처리함)\n","# 데이터셋 갖고옴 True 설정인거만 다운로드\n","train_ds = datasets.MNIST('/tmp/mnist2', train=True,  download=True, transform=tfm)\n","test_ds  = datasets.MNIST('/tmp/mnist2', train=False, download=True, transform=tfm)\n","tr = DataLoader(train_ds, batch_size=256, shuffle=True) # train만 셔플\n","te = DataLoader(test_ds,  batch_size=512, shuffle=False)\n","\n","# smallCNN 만들게(파이토치에서 제공하는 뉴럴네트워크모듈nn.Module 상속받을게)\n","class SmallCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            # feature extraction (특징 추출) 과정\n","            # 안배웠는데 Conv2d 컨볼루션투디 (이미지) 모델 가져올게\n","            # 1개 입력해서 16개 출력할게, 16개가 32개로 가.\n","            # 3은 채널정보(RGB)\n","            # CNN은 2차원 모양을 그대로 간다. (이해 못해도 됨)\n","            # padding을 거치면 크기가 작아진다. (필터?)->\n","            # 다음 은닉층에 사이즈가 틀려버린채로 전달되자나? ->\n","            # (해결) 사이즈는 그대로 두고 다음 은닉층이 받는 사이즈에 빈 공간은 0으로 맞춰줌(zero padding)\n","            # maxpooling : 특징이 있는 것만 뽑겠다.(max만 뽑겠다)\n","            nn.Conv2d(1,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            nn.Conv2d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            # 컴퓨터에게 보내기 위해 flatten으로 1차원으로 변경\n","            # conv2 의 32개 출력이 7x7사이즈 이미지가 됨.(32장)\n","            # 노드 128개(출력값)\n","            nn.Flatten(), nn.Linear(32*7*7, 128), nn.ReLU(), nn.Linear(128,10)\n","        )\n","    # self.net(x) = SmallCNN 구조를 가지게 됨\n","    def forward(self,x): return self.net(x)\n","\n","# 훈련용 데이터를 평가하겠다.\n","def train_eval(criterion):\n","    model = SmallCNN().to(device) # traniset을 smallCNN 모델로. gpu 통과.\n","    opt = optim.AdamW(model.parameters(), lr=2e-3) # 최적화(AdamW 성능좋음)로 파라미터(W,b) 조절하겠다\n","    for _ in range(3):\n","        model.train() # train할게\n","        for x,y in tr:\n","            x,y = x.to(device), y.to(device)\n","            opt.zero_grad()\n","            loss = criterion(model(x), y) # model(x) : 예측값, 손실함수계싼\n","            loss.backward(); opt.step()\n","    model.eval(); correct=0; tot=0 # accuarcy 계산할거다\n","    # 평가용 모드\n","    with torch.no_grad():\n","        for x,y in te:\n","            x,y = x.to(device), y.to(device)\n","            pred = model(x).argmax(1) # 예측값 가장큰걸 넣어라 (0은 loss, 1은 accuracy가 나옴 구조상)\n","            correct += (pred==y).sum().item(); tot += y.size(0) # 전체 total 중 맞은 개수 = accuarcy\n","    return correct/tot\n","\n","# Label smoothing (일반화 성능 향상)\n","crit_ls = nn.CrossEntropyLoss(label_smoothing=0.1)\n","acc_ls = train_eval(crit_ls)\n","\n","# Class weight 예시(0~9 가중치 다르게, 임의)\n","weights = torch.tensor([1,1,1,1,1,1.2,1,1.2,1,1.2], dtype=torch.float32).to(device)\n","crit_w = nn.CrossEntropyLoss(weight=weights)\n","acc_w = train_eval(crit_w)\n","\n","print(f\"MNIST Acc: LabelSmoothing={acc_ls:.3f} | WeightedCE={acc_w:.3f}\")\n"]}]}